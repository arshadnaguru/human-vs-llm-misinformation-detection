# Detecting LLM-Generated Misinformation using Transformer-Based Models

This project investigates whether transformer-based language models can reliably distinguish
between **human-written misinformation** and **misinformation generated by large language models (LLMs)**.
We fine-tune and evaluate RoBERTa-base and DeBERTa-v3-base on the LLMFake dataset to analyze
performance, robustness, and architectural trade-offs.

---

## ğŸ” Motivation
The rapid adoption of large language models has enabled scalable content generation, but it also
raises serious concerns around **AI-generated misinformation**. Detecting such content is
increasingly challenging due to stylistic similarity with human-written text.
This project aims to empirically study whether modern transformer architectures can
serve as reliable detectors.

---

## ğŸ§  Models Used
- **RoBERTa-base**
- **DeBERTa-v3-base**

Both models are fine-tuned for **binary classification**:
- `0` â†’ Human-written misinformation  
- `1` â†’ LLM-generated misinformation  

---

## ğŸ“Š Dataset
We use the **LLMFake dataset** introduced by Chen & Shu (ICLR 2024).

**Sources include:**
- PolitiFact
- GossipCop
- CoAID

**LLMs used for generation:**
- ChatGPT-3.5
- GPT-4
- LLaMA-2
- Vicuna

ğŸ”— Dataset & Code:  
https://github.com/llm-misinformation/llm-misinformation

---

## âš™ï¸ Methodology
- Text preprocessing and dataset consolidation
- Fine-tuning using HuggingFace Transformers
- Binary cross-entropy loss
- Evaluation using Accuracy, F1-score, and ROC-AUC
- Comparative analysis across architectures

---

## ğŸ“ˆ Results Summary

| Model | Accuracy | F1-score | ROC-AUC |
|------|----------|----------|--------|
| RoBERTa-base | ~0.67 | ~0.66 | ~0.71 |
| DeBERTa-v3-base | ~0.68 | ~0.68 | ~0.73 |

Results indicate that while transformer-based detectors outperform random baselines,
detecting AI-generated misinformation remains challenging, especially for paraphrased content.

---

## ğŸš€ How to Run
```bash
pip install -r requirements.txt
python src/train_roberta.py

